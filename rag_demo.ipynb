{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Advanced RAG System Demo\n",
                "\n",
                "### Objectives:\n",
                "1. **System Resource Monitoring**: Track RAM/CPU usage during heavy tasks.\n",
                "2. **Pipeline Execution**: Load data, chunk, index, retrieve, and generate.\n",
                "3. **Evaluation**: Compare Generated Answers vs Reference Answers using **BLEU-4** and **ROUGE-L** metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup Environment & Utils\n",
                "!pip install -r requirements.txt\n",
                "\n",
                "import sys\n",
                "import os\n",
                "import psutil\n",
                "\n",
                "# Ensure src is in python path\n",
                "sys.path.append(os.getcwd())\n",
                "\n",
                "def print_system_usage(stage=\"\"):\n",
                "    process = psutil.Process(os.getpid())\n",
                "    mem_info = process.memory_info()\n",
                "    mem_mb = mem_info.rss / 1024 / 1024\n",
                "    print(f\"[{stage}] Memory: {mem_mb:.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Configuration & Modules\n",
                "from src.rag.config import RAGConfig\n",
                "from src.rag.data_loader import DataLoader\n",
                "from src.rag.chunking import HierarchicalChunker\n",
                "from src.rag.vector_db import VectorDBHandler\n",
                "from src.rag.retriever import HierarchicalRetriever\n",
                "from src.rag.generator import RAGGenerator\n",
                "from src.rag.evaluator import Evaluator\n",
                "\n",
                "config = RAGConfig()\n",
                "print(\"Configuration Loaded.\")\n",
                "print_system_usage(\"Init\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Authenticate with Hugging Face (Required for Gemma Model)\n",
                "from huggingface_hub import login\n",
                "\n",
                "print(\"Please paste your Hugging Face Token when prompted.\")\n",
                "print(\"You can find it here: https://huggingface.co/settings/tokens\")\n",
                "# If you have already logged in via terminal, you can comment this out.\n",
                "login()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data Loading\n",
                "loader = DataLoader(config)\n",
                "\n",
                "# Download Book\n",
                "book_text = loader.download_book()\n",
                "print(f\"Book loaded. Length: {len(book_text)} chars\")\n",
                "\n",
                "# Load QA Pairs\n",
                "qa_pairs = loader.load_qa_pairs()\n",
                "print(f\"Loaded {len(qa_pairs)} QA pairs for testing.\")\n",
                "print_system_usage(\"Data Loading\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hierarchical Chunking\n",
                "chunker = HierarchicalChunker(\n",
                "    parent_chunk_size=config.PARENT_CHUNK_SIZE,\n",
                "    child_chunk_size=config.CHILD_CHUNK_SIZE,\n",
                "    overlap=config.CHUNK_OVERLAP\n",
                ")\n",
                "\n",
                "chunks = chunker.chunk_data(book_text)\n",
                "print(f\"Created {len(chunks['parents'])} parent chunks and {len(chunks['children'])} child chunks.\")\n",
                "\n",
                "parents = chunks['parents']\n",
                "children = chunks['children']\n",
                "print_system_usage(\"Chunking\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Indexing in Qdrant (Local Disk Mode)\n",
                "# CRITICAL: Force cleanup of previous instances to release file locks\n",
                "import gc\n",
                "try:\n",
                "    if 'vdb' in locals():\n",
                "        print(\"Cleaning up previous DB instance...\")\n",
                "        if hasattr(vdb, 'close'):\n",
                "            vdb.close()\n",
                "        del vdb\n",
                "        gc.collect() # Force garbage collection to release file handles\n",
                "except Exception as e:\n",
                "    print(f\"Cleanup warning: {e}\")\n",
                "\n",
                "vdb = VectorDBHandler(config)\n",
                "vdb.create_collection()\n",
                "\n",
                "print(\"Indexing chunks... (this creates embeddings using CPU/GPU)\")\n",
                "vdb.index_chunks(chunks)\n",
                "print_system_usage(\"Indexing\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Components\n",
                "retriever = HierarchicalRetriever(config, vdb, parents)\n",
                "generator = RAGGenerator(config)\n",
                "evaluator = Evaluator()\n",
                "print(\"RAG Components Ready.\")\n",
                "print_system_usage(\"Model Load\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run RAG Loop & Evaluation\n",
                "import pandas as pd\n",
                "\n",
                "# Run on a subset or all pairs\n",
                "test_pairs = qa_pairs[:5] # Testing on first 5 pairs for demo speed\n",
                "results = []\n",
                "\n",
                "print(f\"Running RAG on {len(test_pairs)} queries...\")\n",
                "\n",
                "for i, qa in enumerate(test_pairs):\n",
                "    question = qa['question']\n",
                "    reference = qa['answer1']\n",
                "    \n",
                "    # 1. Retrieve\n",
                "    context = retriever.retrieve_context(question, top_k=config.TOP_K)\n",
                "    \n",
                "    # 2. Generate\n",
                "    generated_answer = generator.generate_answer(question, context)\n",
                "    \n",
                "    # 3. Evaluate\n",
                "    scores = evaluator.evaluate(generated_answer, reference)\n",
                "    \n",
                "    results.append({\n",
                "        \"Question\": question,\n",
                "        \"Generated Answer\": generated_answer,\n",
                "        \"Reference Answer\": reference,\n",
                "        \"BLEU-4\": scores['bleu'],\n",
                "        \"ROUGE-L\": scores['rouge']\n",
                "    })\n",
                "    print(f\".\", end=\"\") # Progress indicator\n",
                "\n",
                "print(\"\\nDone!\")\n",
                "print_system_usage(\"Inference Complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Results Analysis\n",
                "df_results = pd.DataFrame(results)\n",
                "\n",
                "# Calculate Averages\n",
                "avg_bleu = df_results['BLEU-4'].mean()\n",
                "avg_rouge = df_results['ROUGE-L'].mean()\n",
                "\n",
                "print(\"--- Evaluation Summary ---\")\n",
                "print(f\"Average BLEU-4: {avg_bleu:.4f}\")\n",
                "print(f\"Average ROUGE-L: {avg_rouge:.4f}\")\n",
                "\n",
                "# Display Table\n",
                "df_results[['Question', 'Generated Answer', 'BLEU-4', 'ROUGE-L']]"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}